### 1.3 Significance of the Study

Partially random environments often cause exploration agents to either overlook important, learnable regions or become trapped by irreducibly noisy states, a shortcoming observed in prior intrinsic motivation works (Bellemare et al., 2016; Pathak et al., 2019). Identifying and excluding unlearnable transitions is crucial for stable exploration, since ignoring them prevents infinite loops in so-called "noisy TV" scenarios (Raileanu & Rockt√§schel, 2020). The proposed framework addresses this challenge by systematically separating controllable novelty from intrinsic randomness, enabling robust exploration across both discrete and continuous domains. Such a mechanism unifies earlier concepts of novelty-based, prediction-based, and learning-progress-driven intrinsic rewards (Baranes & Oudeyer, 2009; Oudeyer & Kaplan, 2007).

By reducing the overestimation of purely random states, the resulting approach can support real-world applications in robotics and decision-making, where partial randomness and sparse extrinsic rewards frequently hinder learning (Frank et al., 2014; Todorov et al., 2012).
