## Bibliography

Achiam, J., Edwards, H., Amodei, D., & Abbeel, P. (2018). Variational option discovery algorithms. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1807.10299

Baranes, A., & Oudeyer, P. (2009). R-IAC: robust intrinsically motivated exploration and active learning. *IEEE Transactions on Autonomous Mental Development*, *1*(3), 155–169. https://doi.org/10.1109/tamd.2009.2037513

Bellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R. (2016). Unifying Count-Based exploration and intrinsic motivation. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1606.01868

Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., & Efros, A. A. (2018). Large-Scale study of Curiosity-Driven Learning. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1808.04355

Burda, Y., Edwards, H., Storkey, A. J., & Klimov, O. (2018). Exploration by random network distillation. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1810.12894

Chevalier-Boisvert, M., Dai, B., Towers, M., Rodrigo, D. L., Willems, L., Lahlou, S., Pal, S., Castro, P. S., & Terry, J. (2023). Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.2306.13831

Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018). Diversity is All You Need: Learning Skills without a Reward Function. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1802.06070

Frank, M., Leitner, J., Stollenga, M., Förster, A., & Schmidhuber, J. (2014). Curiosity driven reinforcement learning for motion planning on humanoids. *Frontiers in Neurorobotics*, *7*. https://doi.org/10.3389/fnbot.2013.00025

Gregor, K., Rezende, D. J., & Wierstra, D. (2016). Variational intrinsic control. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1611.07507

Groth, O., Wulfmeier, M., Vezzani, G., Dasagi, V., Hertweck, T., Hafner, R., Heess, N., & Riedmiller, M. A. (2021). Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.2109.08603

Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P. (2016). VIME: Variational Information Maximizing Exploration. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1605.09674

Huang, F., Li, W., Cui, J., Fu, Y., & Li, X. (2021). Unified curiosity-Driven learning with smoothed intrinsic reward estimation. *Pattern Recognition*, *123*, 108352. https://doi.org/10.1016/j.patcog.2021.108352

Jarrett, D., Tallec, C., Altché, F., Mesnard, T., Munos, R., & Valko, M. (2022). Curiosity in hindsight: intrinsic exploration in stochastic environments. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.2211.10515

Kakade, S. M. (2001). *A natural policy gradient*. https://papers.nips.cc/paper/2073-a-natural-policy-gradient

Ladosz, P., Weng, L., Kim, M., & Oh, H. (2022). Exploration in deep reinforcement learning: A survey. *Information Fusion*, *85*, 1–22. https://doi.org/10.1016/j.inffus.2022.03.003

Mavor-Parker, A. N., Young, K. A., Barry, C., & Griffin, L. D. (2021). Escaping Stochastic Traps with Aleatoric Mapping Agents. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.2102.04399

Ostrovski, G., Bellemare, M. G., Van Den Oord, A., & Munos, R. (2017). Count-Based Exploration with Neural Density Models. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1703.01310

Oudeyer, P., & Kaplan, F. (2007). What is intrinsic motivation? A typology of computational approaches. *Frontiers in Neurorobotics*, *1*. https://doi.org/10.3389/neuro.12.006.2007

Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven exploration by self-supervised prediction. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1705.05363

Pathak, D., Gandhi, D., & Gupta, A. (2019). Self-Supervised Exploration via disagreement. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1906.04161

Peters, J., & Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients. *Neural Networks*, *21*(4), 682–697. https://doi.org/10.1016/j.neunet.2008.02.003

Raileanu, R., & Rocktäschel, T. (2020). RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.2002.12292

Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., & Pathak, D. (2020). Planning to explore via Self-Supervised world models. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.2005.05960

Szepesvári, C. (2010). Algorithms for reinforcement learning. In *Synthesis lectures on artificial intelligence and machine learning*. https://doi.org/10.1007/978-3-031-01551-9

Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P. (2016). #Exploration: A study of Count-Based exploration for deep reinforcement learning. *arXiv (Cornell University)*. https://doi.org/10.48550/arxiv.1611.04717

Todorov, E., Erez, T., & Tassa, Y. (2012). MuJoCo: A physics engine for model-based control. *2011 IEEE/RSJ International Conference on Intelligent Robots and Systems*. https://doi.org/10.1109/iros.2012.6386109

Tsitsiklis, J., & Van Roy, B. (1997). An analysis of temporal-difference learning with function approximation. *IEEE Transactions on Automatic Control*, *42*(5), 674–690. https://doi.org/10.1109/9.580874
