\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Tasks and environments}
We evaluate exploration and control on five standard Gymnasium benchmarks spanning discrete and continuous control:
\textsc{MountainCar-v0}, \textsc{BipedalWalker-v3}, and the MuJoCo locomotion tasks \textsc{Ant-v5}, \textsc{HalfCheetah-v5}, and \textsc{Humanoid-v5}.
Across all experiments we use the default environment reward functions and termination conditions, with no domain randomization and no additional frame skipping.
Training uses vectorized environments with $B$ parallel instances and rollouts of length $T$ per instance.
We set $(B,T)$ so that the nominal on-policy batch size per PPO update is
\begin{equation}
    N = B \times T = 16{,}384
\end{equation}
transitions, varying $(B,T)$ only to accommodate environment throughput and episode structure.
To match the total step budget exactly, the final update may use a smaller batch when fewer than $N$ steps remain.
Table~\ref{tab:env_suite} summarizes the suite, interaction budgets, and parallelization settings.
For each environment we evaluate every method using the same set of training seeds (Table~\ref{tab:env_suite}) and enable deterministic execution where supported.

\input{tables/env_suite}

\paragraph{Training budgets beyond the early-learning regime.}
The total environment-step budgets in Table~\ref{tab:env_suite} are intentionally generous so that runs extend beyond the initial improvement phase.
This provides headroom to assess long-horizon stability of exploration behavior and intrinsic shaping, including late-training drift, regressions, or oscillations.

\subsection{Methods compared}
\label{sec:experimental_setup:methods}
We compare two proposed intrinsic-reward methods, \textbf{GLPE} and \textbf{GLPE (no gate)} (Section~\ref{sec:method}), against a set of commonly used exploration baselines:
\textbf{Vanilla (extrinsic-only PPO)}, \textbf{ICM}, \textbf{RND}, \textbf{RIDE}, and \textbf{RIAC}.
Vanilla (extrinsic-only PPO) optimizes only the environment reward.
ICM uses forward-model prediction error in a learned feature space as an intrinsic signal, while RND uses the prediction error of a trainable predictor against a fixed random target network.
RIDE rewards feature-space impact, down-weighted by episodic visitation counts obtained by discretizing features, and RIAC uses region-local learning progress computed from changes in forward-model error within an adaptive partition of feature space.
All methods use the same PPO backbone and network architectures; within each environment we keep PPO hyperparameters fixed across methods to isolate the effect of the intrinsic objective.

For intrinsic-reward methods, the policy is trained with the augmented reward defined in Section~\ref{sec:method}, combining the environment reward with a scaled and clipped intrinsic term.
We use the same intrinsic scaling coefficient $\eta$ and clipping range $r_{\max}$ for all intrinsic baselines within a given environment (Table~\ref{tab:glpe_hparams}), so differences arise from the intrinsic signal itself rather than from reward-scale tuning.
GLPE (no gate) is treated as a first-class proposed variant: it uses the same region partitioning, impact term, and regional learning-progress signal as GLPE, but applies no suppression mechanism (i.e., the gate is always active).

\subsection{RL backbone and training protocol}
\label{sec:experimental_setup:training}
All agents are trained with Proximal Policy Optimization (PPO) with generalized advantage estimation (GAE).
The policy and value function are parameterized by separate multilayer perceptrons with two hidden layers of width 256 and ReLU activations.
For continuous-control tasks, the policy outputs a diagonal Gaussian distribution; actions are squashed to the environment bounds when those bounds are finite.

Each training iteration proceeds as follows:
(i) collect up to $N=16{,}384$ on-policy transitions by running the current policy in $B$ vectorized environments for $T$ steps;
(ii) compute the intrinsic reward for each transition (when applicable), set intrinsic rewards to zero on terminal transitions, and form the total reward used for advantage estimation;
(iii) compute advantages and value targets with GAE, bootstrapping across time-limit truncations when a valid final observation is available and treating truncations without a final observation as terminal for bootstrapping purposes; and
(iv) perform multiple epochs of PPO updates over shuffled minibatches of the collected batch.
We use Adam optimizers for both the policy and value networks, normalize advantages within each update batch, and clip gradient norms to 1.0.

We normalize vector observations using a running mean and variance estimated online from collected rollouts, and apply the same normalization to the inputs of the policy, value, and intrinsic modules.

\paragraph{Intrinsic reward computation and updates.}
Intrinsic modules are updated once per PPO iteration using the same on-policy batch used for PPO.
For methods whose intrinsic output is not internally normalized (e.g., ICM, RND, and RIDE), we apply a running RMS normalization to the raw intrinsic signal before scaling and clipping.
For methods that produce a normalized intrinsic output by construction (RIAC and the GLPE family), we directly apply scaling and clipping.
For GLPE and GLPE (no gate), we additionally anneal the intrinsic coefficient $\eta_t$ using the cosine taper described in Section~\ref{sec:method}, with task-specific start and end fractions listed in Table~\ref{tab:glpe_hparams}.

\subsection{Evaluation protocol}
\label{sec:experimental_setup:evaluation}
We evaluate performance using undiscounted episodic return from the environment (extrinsic reward only).
We do not run evaluation rollouts online during training; instead, we evaluate saved checkpoints offline.
Each evaluated checkpoint is run for 20 episodes in a separate environment instance using a deterministic action mode (mode/mean action for stochastic policies).
Episodes are reset with fixed per-episode seeds derived from the training seed.
For each environment and method, we run multiple independent training seeds (Table~\ref{tab:env_suite}) and aggregate results across seeds.

\paragraph{Checkpointing and logging.}
During training we checkpoint the full agent state at step 0, at nine additional warmup points approximately evenly spaced within the first checkpoint interval, and then at a fixed \texttt{checkpoint\_interval} thereafter (set to 10\% of the total step budget per environment), with a final checkpoint at the end of training.
We additionally log scalar metrics every fixed number of environment steps (set to 2\% of the total step budget).

\subsection{Efficiency measurements}
\label{sec:experimental_setup:efficiency}
In addition to sample efficiency (learning as a function of environment interaction), we track computational efficiency using wall-clock time.
All timings are measured in the same execution setting used for training (CPU in our experiments) and therefore reflect end-to-end optimization overhead.
For each PPO iteration we log per-component wall-clock time spent in environment interaction, policy inference, intrinsic computation, intrinsic-module updates, advantage computation, and PPO optimization, and we use these logs to form cumulative training time.

When reporting scalar summaries of learning curves, we compute area-under-the-curve (AUC) metrics via trapezoidal integration of mean return as a function of either environment steps or wall-clock time.
For time-based summaries, we use a \emph{common time budget} per environment defined as the minimum final wall-clock time attained among the compared methods, ensuring that summaries do not rely on extrapolation beyond the measured runtime of any method.

\subsection{Implementation details and hyperparameters}
\label{sec:experimental_setup:impl_details}
Tables~\ref{tab:ppo_hparams} and~\ref{tab:glpe_hparams} list the PPO and GLPE hyperparameters used in all experiments.
Unless specified in the tables, we use $\gamma=0.99$ and value-loss coefficient 0.5 for PPO.

For methods that learn a latent dynamics model (ICM, RIDE, RIAC, and the GLPE family), the shared encoder produces 128-dimensional features and is implemented as a two-layer MLP (256 units per layer, ReLU) for these vector-observation tasks.
Forward and inverse model losses are weighted equally, the model is trained with Adam at learning rate $3\times 10^{-4}$, and intrinsic-model gradient norms are clipped to 5.0.

\input{tables/ppo_hparams}

\input{tables/glpe_hparams}
