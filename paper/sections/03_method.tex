\section{Method}
\label{sec:method}

\subsection{Problem setting}

We consider an episodic Markov decision process (MDP) with states $s \in \mathcal{S}$, actions
$a \in \mathcal{A}$, transition dynamics $P(s_{t+1}\mid s_t,a_t)$, and extrinsic reward
$r^{\text{ext}}_t = r^{\text{ext}}(s_t,a_t,s_{t+1})$. A stochastic policy $\pi_\theta(a\mid s)$
is trained to maximize the discounted return with discount $\gamma \in (0,1]$.
To improve exploration, we augment the extrinsic reward with an intrinsic signal
$r^{\text{int}}_t$ computed online from the agent's experience.

\subsection{Policy optimization}

We optimize $\pi_\theta$ with Proximal Policy Optimization (PPO). Given a batch of transitions
collected under the current policy, we form the clipped surrogate objective
\begin{equation}
\mathcal{L}_{\text{PPO}}(\theta)
=
\mathbb{E}\Big[
\min\big(
\rho_t(\theta)\,\hat{A}_t,\;
\text{clip}(\rho_t(\theta), 1-\epsilon, 1+\epsilon)\,\hat{A}_t
\big)
\Big],
\end{equation}
where $\rho_t(\theta) = \exp(\log \pi_\theta(a_t\mid s_t) - \log \pi_{\theta_{\text{old}}}(a_t\mid s_t))$,
$\hat{A}_t$ is an advantage estimate, and $\epsilon$ is the PPO clip range. We jointly train a value
function $V_\psi(s)$ by minimizing a squared-error loss to value targets (optionally with value
clipping), and we include an entropy bonus to encourage exploration:
\begin{equation}
\mathcal{J}(\theta,\psi) = \mathcal{L}_{\text{PPO}}(\theta)
- c_{\text{ent}}\,\mathbb{E}[\mathcal{H}(\pi_\theta(\cdot\mid s_t))]
- c_V\,\mathbb{E}\big[(V_\psi(s_t)-\hat{V}_t)^2\big].
\end{equation}
To stabilize updates, we normalize advantages within each PPO update, apply gradient clipping,
and optionally stop early when an approximate KL divergence exceeds a configured threshold.

\subsection{Advantage estimation with time-limit bootstrapping}

We compute advantages using Generalized Advantage Estimation (GAE). Let $r_t$ denote the reward used
for learning (defined below). For nonterminal transitions,
\begin{equation}
\delta_t = r_t + \gamma V_\psi(s_{t+1}) - V_\psi(s_t), \qquad
\hat{A}_t = \sum_{\ell \ge 0} (\gamma\lambda)^\ell \delta_{t+\ell},
\end{equation}
with GAE parameter $\lambda \in [0,1]$. When an episode ends due to a time limit (truncation),
we bootstrap from the value estimate at the final observation rather than forcing the return to
zero. For true terminals, we do not bootstrap across the boundary.

\subsection{Intrinsic-reward augmented learning signal}

Training uses the combined reward
\begin{equation}
r_t = r^{\text{ext}}_t + \eta \, w_t \, \text{clip}\!\left(r^{\text{int}}_t,\, -r_{\text{clip}},\, r_{\text{clip}}\right),
\label{eq:reward_total}
\end{equation}
where $\eta \ge 0$ scales intrinsic motivation and $r_{\text{clip}} > 0$ limits its magnitude.
For the proposed methods, $r^{\text{int}}_t$ is already normalized (Section~\ref{sec:glpe}),
so Eq.~\eqref{eq:reward_total} applies clipping and scaling directly.

\paragraph{Intrinsic taper.}
Optionally, we anneal intrinsic influence with a cosine taper $w_t \in [0,1]$ over the course of
training: $w_t=1$ initially, then decreases smoothly between a start fraction and end fraction of
total environment steps, and remains $0$ thereafter. This encourages exploration early while allowing
asymptotic exploitation to be driven primarily by the extrinsic objective.

\subsection{Representation learning via ICM}

Several intrinsic signals in our system rely on a learned state embedding $\phi(s)\in\mathbb{R}^d$.
We learn $\phi$ using an Intrinsic Curiosity Module (ICM): an encoder produces $\phi(s)$, a forward
model predicts the next embedding, and an inverse model predicts the action given consecutive
embeddings. For a transition $(s_t,a_t,s_{t+1})$, the forward prediction error is
\begin{equation}
e_t \;=\; \frac{1}{d}\left\| f(\phi(s_t), a_t) - \phi(s_{t+1}) \right\|_2^2,
\label{eq:icm_forward_error}
\end{equation}
and the inverse model loss encourages $\phi$ to be action-relevant. The ICM parameters are updated
online alongside the policy. The same encoder $\phi$ is used by our proposed exploration methods.

\subsection{Adaptive region partitioning in latent space}

To reason about competence and progress locally, we partition the embedding space into regions.
Each embedding $\phi(s_t)$ is inserted into an incremental KD-tree that recursively splits leaf
regions when their sample count exceeds a capacity. Splits are chosen along high-variance embedding
dimensions and use median thresholds, yielding an adaptive discretization of the visited portion of
latent space. Each leaf is assigned a persistent region index $i \in \{0,1,\dots\}$, and each
transition is associated with the region containing $\phi(s_t)$.

\subsection{Learning progress in a region}

For each region $i$, we maintain exponential moving averages (EMAs) of the ICM forward error
$e_t$ (Eq.~\eqref{eq:icm_forward_error}) using a \emph{long} and \emph{short} timescale:
\begin{equation}
\bar{e}^{\text{long}}_i \leftarrow \beta_{\text{long}}\,\bar{e}^{\text{long}}_i + (1-\beta_{\text{long}})\,e_t,
\qquad
\bar{e}^{\text{short}}_i \leftarrow \beta_{\text{short}}\,\bar{e}^{\text{short}}_i + (1-\beta_{\text{short}})\,e_t.
\end{equation}
We define \emph{learning progress} (LP) as the positive part of the difference between these EMAs:
\begin{equation}
\text{LP}_i \;=\; \max\!\left(0,\;\bar{e}^{\text{long}}_i - \bar{e}^{\text{short}}_i\right).
\label{eq:lp_def}
\end{equation}
Intuitively, $\text{LP}_i$ is large when prediction error is decreasing quickly in region $i$,
indicating rapid learning (competence acquisition), and near zero when learning has saturated.

\subsection{Impact as novelty}

We quantify instantaneous novelty by the embedding-space displacement between consecutive states:
\begin{equation}
\text{Impact}_t \;=\; \left\|\phi(s_{t+1}) - \phi(s_t)\right\|_2.
\label{eq:impact_def}
\end{equation}
This signal is large when the transition moves the agent to a substantially different part of the
representation space.

\subsection{Gated Learning-Progress Exploration (GLPE)}
\label{sec:glpe}

GLPE combines novelty and competence into a single intrinsic reward while \emph{gating} the signal
in regions that appear unproductive. The method maintains region-level LP (Eq.~\eqref{eq:lp_def})
and uses robust global statistics across regions to decide when a region should be gated off.

\paragraph{Global medians.}
Let $\mathcal{I}$ be the set of regions that have been visited at least once. We compute
\begin{equation}
\widetilde{\text{LP}} = \text{median}_{i\in\mathcal{I}} \text{LP}_i,
\qquad
\widetilde{e} = \text{median}_{i\in\mathcal{I}} \bar{e}^{\text{short}}_i.
\end{equation}
We use $\widetilde{\text{LP}}$ to scale a learning-progress threshold and $\widetilde{e}$ to normalize
region ``surprise.''

\paragraph{Region gating rule.}
For region $i$, define a normalized surprise score
\begin{equation}
s_i = \frac{\bar{e}^{\text{short}}_i}{\widetilde{e} + \varepsilon},
\end{equation}
with small $\varepsilon>0$ for numerical stability. Let $\tau_s$ be a surprise threshold and let
$\tau_{\text{LP}} = \tau_{\text{lp}}\widetilde{\text{LP}}$ where $\tau_{\text{lp}}$ is a multiplier.
A region is considered \emph{bad} when it exhibits low learning progress but high surprise:
\begin{equation}
\text{bad}_i \;\equiv\; \left(\text{LP}_i < \tau_{\text{LP}}\right)\;\wedge\;\left(s_i > \tau_s\right).
\label{eq:bad_condition}
\end{equation}
GLPE uses persistence counters to avoid chattering: when a region is currently \emph{on}, it is
switched \emph{off} only after $\text{bad}_i$ holds for a minimum number of consecutive updates.
When a region is \emph{off}, it is switched \emph{on} after sustained recovery, implemented as
requiring $\text{LP}_i$ to exceed a hysteresis-scaled threshold $h\,\tau_{\text{LP}}$ for multiple
consecutive updates (with $h>1$). Gating is only activated once a minimum number of regions have been
populated; before that, all regions remain on.

We denote the resulting binary gate for region $i$ by $g_i \in \{0,1\}$.

\paragraph{Component normalization and intrinsic reward.}
Because Impact and LP can have very different scales across environments and throughout training,
GLPE normalizes each component with an online running root-mean-square (RMS) estimate. Let
$\text{RMS}_{\text{imp}}$ and $\text{RMS}_{\text{lp}}$ be running RMS values for Impact and LP,
updated online with an exponential moving average. Define normalized components
\begin{equation}
\widehat{\text{Impact}}_t = \frac{\text{Impact}_t}{\text{RMS}_{\text{imp}}}, \qquad
\widehat{\text{LP}}_i = \frac{\text{LP}_i}{\text{RMS}_{\text{lp}}}.
\end{equation}
The GLPE intrinsic reward for transition $t$ in region $i$ is then
\begin{equation}
r^{\text{int}}_t
\;=\;
g_i\left(\alpha_{\text{imp}}\widehat{\text{Impact}}_t + \alpha_{\text{lp}}\widehat{\text{LP}}_i\right),
\label{eq:glpe_reward}
\end{equation}
where $\alpha_{\text{imp}}\ge 0$ and $\alpha_{\text{lp}}\ge 0$ weight novelty and competence.

\subsection{GLPE (no gate)}

Our second proposed method, \textbf{GLPE (no gate)}, removes the gating mechanism while keeping the
same region construction, learning-progress computation, and component normalization. Concretely, we
set $g_i \equiv 1$ for all regions, so the intrinsic reward becomes
\begin{equation}
r^{\text{int}}_t
\;=\;
\alpha_{\text{imp}}\widehat{\text{Impact}}_t + \alpha_{\text{lp}}\widehat{\text{LP}}_i.
\label{eq:glpe_nogate_reward}
\end{equation}
This variant isolates the contribution of the gating rule itself: any performance differences
between GLPE and GLPE (no gate) reflect the effect of selectively suppressing intrinsic rewards in
regions that appear to be noisy (high surprise) yet unproductive (low learning progress).

\subsection{Computational considerations}

The global medians $(\widetilde{\text{LP}}, \widetilde{e})$ are computed across visited regions and
can be cached and updated periodically to reduce overhead. When recomputed every update, the gating
rule in Eq.~\eqref{eq:bad_condition} is exact; when cached, it is evaluated using slightly stale
global statistics, providing a controllable trade-off between fidelity and speed. In all cases,
intrinsic rewards are computed online during rollout collection and incorporated into PPO via
Eq.~\eqref{eq:reward_total}.
