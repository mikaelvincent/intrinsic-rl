\section{Method}
\label{sec:method}

\subsection{Problem setup}
We consider an episodic Markov decision process with observations $o_t \in \mathcal{O}$, actions $a_t \in \mathcal{A}$, and extrinsic rewards $r^{\mathrm{ext}}_t$. A stochastic policy $\pi_\theta(a\mid o)$ is trained to maximize the expected discounted return.

We augment the extrinsic reward with an intrinsic term computed from single-step transitions,
\begin{equation}
    r_t = r^{\mathrm{ext}}_t + \eta_t\,\mathrm{clip}\big(r^{\mathrm{int}}_t, -r_{\max}, r_{\max}\big),
\end{equation}
where $\eta_t\ge 0$ controls the intrinsic--extrinsic trade-off and $r_{\max}>0$ bounds the magnitude of intrinsic shaping. For terminal transitions we set $r_t^{\mathrm{int}}=0$ so that intrinsic shaping does not depend on episode termination.

We propose two intrinsic reward formulations within a shared framework: \textbf{GLPE (no gate)} and \textbf{GLPE}. Both combine feature-space impact with region-local learning progress; GLPE additionally introduces a region-specific gate that suppresses intrinsically rewarding but unproductive experience.

\subsection{Latent dynamics model}
Let $\phi_\omega$ be a learned encoder producing a $d$-dimensional feature vector $z_t = \phi_\omega(o_t)$. We train (i) a forward dynamics predictor $f_\psi$ that maps $(z_t,a_t)$ to a prediction of the next feature vector, and (ii) an inverse dynamics predictor $g_\psi$ that maps $(z_t,z_{t+1})$ to an action distribution. The inverse model is instantiated as a classifier for discrete actions and as a Gaussian likelihood model for continuous actions.

Training uses the combined loss
\begin{equation}
    \mathcal{L}_{\mathrm{dyn}}(t) = \beta_{\mathrm{fwd}}\,\mathcal{L}_{\mathrm{fwd}}(t) + \beta_{\mathrm{inv}}\,\mathcal{L}_{\mathrm{inv}}(t),
\end{equation}
with weights $\beta_{\mathrm{fwd}},\beta_{\mathrm{inv}}>0$. The forward loss is the mean-squared error in feature space,
\begin{equation}
    \mathcal{L}_{\mathrm{fwd}}(t) = \frac{1}{d}\left\lVert f_\psi(z_t,a_t) - z_{t+1} \right\rVert_2^2,
\end{equation}
and we define the per-transition prediction error
\begin{equation}
    e_t = \mathcal{L}_{\mathrm{fwd}}(t).
\end{equation}
The intrinsic rewards below depend on $(z_t,z_{t+1})$ and $e_t$, but do not require access to privileged environment state.

\subsection{Online region partitioning in latent space}
To localize learning progress, we maintain an online partition of the latent space into regions. Each region corresponds to a leaf in a binary space-partitioning tree over the $d$-dimensional feature space.

Given an embedding $z_t$, we assign it to a leaf region $\rho_t$ by routing it through the tree. Each leaf maintains a visit counter and a small buffer of recently assigned points. When the number of assigned points to a leaf reaches a capacity $C$ and the leaf depth is below a maximum depth $D_{\max}$, the leaf is split: we choose the split dimension as the coordinate with highest empirical variance among buffered points and split at the median value along that coordinate. This procedure yields a growing set of regions that adaptively refines frequently visited parts of feature space.

Region statistics (defined next) are updated using the region assignment based on the current embedding $z_t$.

\subsection{Regional learning progress}
For each region $r$ we maintain two exponential moving averages (EMAs) of the forward prediction error: a long-horizon average $\mu^{\mathrm{long}}_r$ and a short-horizon average $\mu^{\mathrm{short}}_r$. When a transition at time $t$ is assigned to region $\rho_t=r$, we update
\begin{align}
    \mu^{\mathrm{long}}_r &\leftarrow \beta_{\mathrm{long}}\,\mu^{\mathrm{long}}_r + (1-\beta_{\mathrm{long}})\,e_t,\\
    \mu^{\mathrm{short}}_r &\leftarrow \beta_{\mathrm{short}}\,\mu^{\mathrm{short}}_r + (1-\beta_{\mathrm{short}})\,e_t,
\end{align}
with $0<\beta_{\mathrm{long}}<1$, $0<\beta_{\mathrm{short}}<1$, and typically $\beta_{\mathrm{long}}>\beta_{\mathrm{short}}$ so that $\mu^{\mathrm{long}}_r$ changes more slowly. On the first visit to a region, we initialize $\mu^{\mathrm{long}}_r=\mu^{\mathrm{short}}_r=e_t$.

We define region-wise learning progress as the positive part of the discrepancy between the long and short averages,
\begin{equation}
    \mathrm{LP}_t = \max\big(0,\,\mu^{\mathrm{long}}_{\rho_t}-\mu^{\mathrm{short}}_{\rho_t}\big).
\end{equation}
This quantity is large when the short-term error has recently dropped below the longer-term baseline, indicating active model improvement in that region.

\subsection{Impact in feature space}
We also compute a feature-space impact term that encourages transitions that change the learned representation,
\begin{equation}
    I_t = \left\lVert z_{t+1}-z_t \right\rVert_2.
\end{equation}
Unlike count-based novelty, impact depends on the learned representation and can capture structured changes in high-dimensional observations.

\subsection{Component-wise scale stabilization}
The magnitudes of $I_t$ and $\mathrm{LP}_t$ can vary across tasks and throughout training. To obtain a stable scale without task-specific tuning, we maintain running root-mean-square (RMS) normalizers for each component using an exponential average of squared values. For a scalar signal $x_t$ we update an accumulator
\begin{equation}
    v \leftarrow \beta_{\mathrm{rms}}\,v + (1-\beta_{\mathrm{rms}})\,x_t^2,
\end{equation}
and normalize by $\mathrm{RMS}(x_t)=\sqrt{v+\varepsilon}$ with a small $\varepsilon>0$.

Applying this independently to $I_t$ and $\mathrm{LP}_t$ yields normalized components
\begin{equation}
    \widetilde{I}_t = \frac{I_t}{\mathrm{RMS}(I_t)},\qquad
    \widetilde{\mathrm{LP}}_t = \frac{\mathrm{LP}_t}{\mathrm{RMS}(\mathrm{LP}_t)}.
\end{equation}
We then define a shared base intrinsic score
\begin{equation}
    u_t = \alpha_{\mathrm{impact}}\,\widetilde{I}_t + \alpha_{\mathrm{LP}}\,\widetilde{\mathrm{LP}}_t,
\end{equation}
with nonnegative weights $\alpha_{\mathrm{impact}},\alpha_{\mathrm{LP}}$. Both proposed methods use the same $u_t$ and differ only in whether it is gated.

\subsection{GLPE (no gate)}
\textbf{GLPE (no gate)} is a region-aware intrinsic reward that combines feature impact and regional learning progress without any explicit suppression mechanism. The intrinsic reward is
\begin{equation}
    r_t^{\mathrm{int}} = u_t.
\end{equation}
This variant preserves the locality and nonstationarity tracking provided by the region partition and EMAs, while avoiding additional gating hyperparameters.

\subsection{GLPE: gated learning-progress exploration}
\textbf{GLPE} augments the same base score $u_t$ with a binary, region-specific gate that suppresses intrinsic rewards in regions that appear difficult to predict yet show little or no learning progress.

\paragraph{Global robust thresholds.}
At any time, let $\mathcal{R}$ denote the set of regions that have been visited at least once. For each $r\in\mathcal{R}$ define the current regional learning progress
$\mathrm{LP}(r)=\max\big(0,\mu^{\mathrm{long}}_r-\mu^{\mathrm{short}}_r\big)$.
We compute robust global reference values using medians across visited regions,
\begin{equation}
    \mathrm{LP}_{\mathrm{med}} = \mathrm{median}_{r\in\mathcal{R}}\,\mathrm{LP}(r),
    \qquad
    e_{\mathrm{med}} = \mathrm{median}_{r\in\mathcal{R}}\,\mu^{\mathrm{short}}_r.
\end{equation}
We define a learning-progress threshold $\tau_{\mathrm{LP}} = \kappa\,\mathrm{LP}_{\mathrm{med}}$ with multiplier $\kappa>0$, and a scale-free normalized error score
\begin{equation}
    s_r = \frac{\mu^{\mathrm{short}}_r}{e_{\mathrm{med}} + \varepsilon},
\end{equation}
where $\varepsilon>0$ avoids division by zero. A region $r$ is considered \emph{unproductive} at a visit if it simultaneously has low learning progress and high normalized error,
\begin{equation}
    \mathrm{LP}(r) < \tau_{\mathrm{LP}} \quad\text{and}\quad s_r > \tau_s,
\end{equation}
with $\tau_s>0$ a fixed threshold.

\paragraph{Gate dynamics.}
Each region maintains a gate value $g_r\in\{0,1\}$ and two small counters tracking consecutive unproductive and productive visits.
Gating is only activated once at least $R_{\min}$ regions have been visited so that the medians are meaningful.
When gating is active, the update uses persistence and hysteresis:
(i) if $g_r=1$ and the region is unproductive for $K$ consecutive visits, set $g_r\leftarrow 0$;
(ii) if $g_r=0$, re-enable the region when $\mathrm{LP}(r) > h\,\tau_{\mathrm{LP}}$ for two consecutive visits, where $h>1$ is a hysteresis multiplier.
If gating is inactive (fewer than $R_{\min}$ visited regions), we set $g_r\leftarrow 1$ and reset the counters.

The intrinsic reward for GLPE is then
\begin{equation}
    r_t^{\mathrm{int}} = g_{\rho_t}\,u_t.
\end{equation}

\subsection{On-policy optimization and intrinsic scheduling}
We optimize the policy using an on-policy actor--critic method with clipped policy updates and generalized advantage estimation. At each iteration we collect rollouts under the current policy, compute intrinsic rewards for each transition, and form the total rewards $r_t$ used for advantage estimation and policy/value updates.

For GLPE-based methods, we optionally anneal the strength of intrinsic shaping over the course of training using a cosine taper. Let $p\in[0,1]$ denote training progress as a fraction of total environment steps. Given a start fraction $p_{\mathrm{start}}$ and end fraction $p_{\mathrm{end}}$ with $p_{\mathrm{start}}<p_{\mathrm{end}}$, we define
\begin{equation}
    w(p) =
    \begin{cases}
        1, & p \le p_{\mathrm{start}},\\
        \tfrac{1}{2}\big(1+\cos(\pi\,\tfrac{p-p_{\mathrm{start}}}{p_{\mathrm{end}}-p_{\mathrm{start}}})\big), & p_{\mathrm{start}}<p<p_{\mathrm{end}},\\
        0, & p \ge p_{\mathrm{end}},
    \end{cases}
\end{equation}
and set $\eta_t=\eta\,w(p_t)$ in the augmented reward.

The latent dynamics model parameters $(\omega,\psi)$ are updated using the same on-policy transitions that generate intrinsic rewards, while the region statistics, gates, and RMS normalizers are updated online during intrinsic computation.

\begin{center}
\begin{minipage}{0.97\linewidth}
\textbf{Algorithm 1: Intrinsic reward for GLPE variants}\\
\begin{tabular}{l}
\hline
\textbf{Input:} transition $(o_t,a_t,o_{t+1})$; encoder $\phi_\omega$; forward model $f_\psi$; region map $\rho$; \\
\quad region EMAs $\{\mu^{\mathrm{long}}_r,\mu^{\mathrm{short}}_r\}$; RMS accumulators for $I_t$ and $\mathrm{LP}_t$; gate states $\{g_r\}$ (GLPE only).\\
\textbf{1.} Compute $z_t=\phi_\omega(o_t)$ and $z_{t+1}=\phi_\omega(o_{t+1})$.\\
\textbf{2.} Compute forward error $e_t=\frac{1}{d}\|f_\psi(z_t,a_t)-z_{t+1}\|_2^2$.\\
\textbf{3.} Assign region $\rho_t\leftarrow\rho(z_t)$ (inserting $z_t$ and splitting leaves when needed).\\
\textbf{4.} Update $(\mu^{\mathrm{long}}_{\rho_t},\mu^{\mathrm{short}}_{\rho_t})$ using $e_t$ and compute $\mathrm{LP}_t=\max(0,\mu^{\mathrm{long}}_{\rho_t}-\mu^{\mathrm{short}}_{\rho_t})$.\\
\textbf{5.} Compute impact $I_t=\|z_{t+1}-z_t\|_2$.\\
\textbf{6.} Update RMS accumulators for $I_t$ and $\mathrm{LP}_t$ and compute $(\widetilde{I}_t,\widetilde{\mathrm{LP}}_t)$.\\
\textbf{7.} Form $u_t=\alpha_{\mathrm{impact}}\widetilde{I}_t+\alpha_{\mathrm{LP}}\widetilde{\mathrm{LP}}_t$.\\
\textbf{8.} \emph{GLPE:} update $g_{\rho_t}$ using median-based thresholds and hysteresis, return $r_t^{\mathrm{int}}=g_{\rho_t}u_t$.\\
\textbf{9.} \emph{GLPE (no gate):} return $r_t^{\mathrm{int}}=u_t$.\\
\hline
\end{tabular}
\end{minipage}
\end{center}
