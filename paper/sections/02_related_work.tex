\section{Related Work}
\label{sec:related_work}

Intrinsic motivation and curiosity provide reinforcement learning agents with internally generated reward signals that guide exploration when extrinsic feedback is sparse or delayed. Early computational accounts formalize curiosity in terms of progress in prediction or compression, encouraging experience that is learnable but not yet mastered \cite{schmidhuber-1991,oudeyer-2007A,oudeyer-2007B,barto-2012}. In deep RL, intrinsic rewards are commonly added to the environment reward as an auxiliary bonus; while potential-based reward shaping can preserve optimal policies under specific conditions \cite{ng-1999}, curiosity bonuses are typically used to bias exploration rather than to provide invariance guarantees.

Exploration has also been formalized through visitation and uncertainty. Classic count-based methods are effective in small or discretized state spaces and have been extended to high-dimensional observations via pseudo-counts from learned density models and hashing/discretization schemes \cite{bellemare-2016,ostrovski-2017,tang-2016}. Because our benchmarks emphasize continuous control with vector observations (Section~\ref{sec:experimental_setup}), we focus on intrinsic signals computed in a learned feature space rather than explicit state counting.

Prediction-error bonuses reward transitions that are surprising under a learned predictive model. Stadie et al.\ assign intrinsic reward from the error of a learned dynamics predictor \cite{stadie-2015}; ICM couples an inverse model (to learn features) with a forward model whose prediction error drives curiosity \cite{pathak-2017}; and Random Network Distillation (RND) measures novelty as predictor error against a fixed random target network \cite{burda-2018B}. Another line derives intrinsic reward from uncertainty or information-theoretic criteria, e.g., information gain about dynamics parameters \cite{houthooft-2016}, mutual-information/empowerment objectives \cite{mohamed-2015}, or disagreement across predictive models \cite{pathak-2019}. A known limitation of raw prediction error is that it can remain high in stochastic regions, yielding the ``noisy-TV'' failure mode where agents pursue irreducible surprise \cite{burda-2018A,burda-2018B,mavor-parker-2021}.

Learning-progress methods address this by rewarding improvement rather than absolute error. Region-based approaches from developmental robotics and intrinsically motivated RL estimate progress locally within an adaptive partition and focus exploration where prediction error is decreasing \cite{singh-2004,oudeyer-2007B,baranes-2009}. Our RIAC baseline instantiates this principle in feature space, and the GLPE family follows the same regional progress signal while additionally incorporating a normalized feature-space impact term. GLPE further introduces a simple region-level gate, using robust global statistics and hysteresis to suppress persistently high-error, low-progress regions (Section~\ref{sec:method}).

Impact-driven exploration provides a complementary notion of novelty grounded in controllable change. RIDE rewards representation change and uses episodic visitation statistics to maintain exploration pressure \cite{raileanu-2020}; our impact component plays an analogous role but is used in concert with learning progress. To isolate the effect of the intrinsic objective, we evaluate all methods under a shared PPO+GAE backbone \cite{schulman-2017,schulman-2015}.
