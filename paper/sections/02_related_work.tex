\section{Related Work}
\label{sec:related_work}

Intrinsic motivation and curiosity provide reinforcement learning agents with internally generated reward signals that guide exploration when extrinsic feedback is sparse or delayed. Early computational accounts formalize curiosity in terms of progress in prediction or compression, encouraging experience that is learnable but not yet mastered \cite{schmidhuber-1991,oudeyer-2007A,oudeyer-2007B,barto-2012}. In deep RL, intrinsic rewards are commonly added to the environment reward as an exploration bonus; unlike potential-based shaping, such bonuses generally do not guarantee policy invariance \cite{ng-1999}.

Exploration has also been formalized through visitation and uncertainty. Classic count-based methods are effective in small or discretized state spaces and have been extended to high-dimensional observations via pseudo-counts, hashing/discretization, and learned density models \cite{bellemare-2016,tang-2016,ostrovski-2017}. Our focus is on vector-observation control benchmarks (Section~\ref{sec:experimental_setup}), where intrinsic signals are more naturally computed in a learned feature space than through explicit state counting.

Prediction-error bonuses reward transitions that are surprising under a learned predictive model. Stadie et al.\ assign intrinsic reward from the error of a learned dynamics predictor \cite{stadie-2015}; ICM couples an inverse model with a forward model whose prediction error drives curiosity \cite{pathak-2017}; and Random Network Distillation (RND) measures novelty as predictor error against a fixed random target network \cite{burda-2018B}. Information-theoretic and ensemble-based objectives instead reward epistemic uncertainty or information gain \cite{houthooft-2016,mohamed-2015,pathak-2019}. A known limitation of raw prediction error is that it can remain high in stochastic or irreducible regions, yielding the ``noisy-TV'' failure mode where agents pursue uninformative surprise \cite{burda-2018A,burda-2018B,mavor-parker-2021}.

Learning-progress methods address this by rewarding improvement rather than absolute error. In developmental robotics, IAC-style approaches estimate progress locally and focus exploration on regions that are neither trivial nor unlearnable \cite{oudeyer-2007B,baranes-2009}. Motivated by these ideas, we propose the \emph{GLPE family} (Section~\ref{sec:method}). \textbf{GLPE} combines region-local learning progress in a learned feature space with an impact-style term as in RIDE \cite{raileanu-2020} and introduces a region-specific gate to suppress persistently high-error, low-progress regions; \textbf{GLPE (no gate)} removes this gate while keeping the same base intrinsic signal.
