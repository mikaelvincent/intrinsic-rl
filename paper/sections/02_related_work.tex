\section{Related Work}
\label{sec:related_work}

Intrinsic motivation and curiosity provide internally generated rewards that encourage exploration when extrinsic feedback is sparse or delayed. Early computational views connect curiosity to model learning by rewarding improvements in prediction or compression, producing a drive toward experiences that are neither trivial nor wholly unlearnable \cite{schmidhuber-1991,oudeyer-2007A,oudeyer-2007B}. In deep reinforcement learning, such signals are typically used as reward shaping and optimized jointly with a standard control objective \cite{pathak-2017,burda-2018B}.

A prominent family of intrinsic rewards uses prediction error as a novelty signal. The Intrinsic Curiosity Module (ICM) learns an encoder together with forward and inverse dynamics predictors and uses forward-model error in feature space as intrinsic reward \cite{pathak-2017}. Random Network Distillation (RND) replaces explicit dynamics prediction with the error of a predictor network trained to match a fixed random target network, yielding a simple and scalable bonus \cite{burda-2018B}. While effective, error-based curiosity can overvalue stochastic or otherwise hard-to-predict transitions, leading to exploration traps and unstable late-training behavior; large-scale studies document these limitations and motivate replacing raw error with signals that reflect learning progress \cite{burda-2018A,pathak-2017}.

Learning-progress approaches reward improvement rather than surprise. In developmental robotics, Intelligent Adaptive Curiosity (IAC) and its robust variants estimate progress locally by partitioning the sensorimotor space and focusing exploration on regions where prediction error decreases over time \cite{oudeyer-2007B,baranes-2009}. This locality helps disambiguate genuinely learnable structure from persistent unpredictability. GLPE adopts the same principle but applies it in a learned latent space using lightweight exponential moving averages to track short- and long-horizon error, making the progress signal compatible with the on-policy data stream used to train the policy and the latent dynamics model (Section~\ref{sec:method}). Our region-specific gate further targets the ``unlearnable but attractive'' regime by suppressing regions that remain high-error without progress, complementing the use of progress itself.

Impact-driven exploration provides a complementary notion of novelty based on controllable change. RIDE rewards the magnitude of change in a learned representation and down-weights revisits using episodic counts, encouraging diverse interactions even when absolute prediction error is small \cite{raileanu-2020}. GLPE includes a normalized impact term alongside regional learning progress, allowing exploration pressure to reflect both representation change and model improvement without relying on task-specific reward rescaling (Section~\ref{sec:experimental_setup}).

Finally, intrinsic reward design is largely orthogonal to the choice of policy optimizer. We evaluate all methods with a common PPO backbone and generalized advantage estimation, enabling controlled comparisons of intrinsic objectives under a standard on-policy training pipeline \cite{schulman-2017,schulman-2015}.
