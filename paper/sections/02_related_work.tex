\section{Related Work}
\label{sec:related_work}

Intrinsic motivation and curiosity introduce internally generated rewards that encourage exploration when extrinsic feedback is sparse or delayed. Early computational accounts frame curiosity as seeking experiences that improve prediction or compression, balancing novelty against unlearnable randomness \cite{schmidhuber-1991,oudeyer-2007A,oudeyer-2007B,singh-2004}. In reinforcement learning, such signals are commonly implemented as reward shaping: the environment reward is augmented during training while evaluation remains based on extrinsic return \cite{ng-1999}. Modern exploration bonuses span approximate visitation methods (e.g., pseudo-counts, density models, and hashing) \cite{bellemare-2016,ostrovski-2017,tang-2016} and uncertainty- or information-gain objectives built on learned dynamics and randomized value functions \cite{houthooft-2016,osband-2016,plappert-2017}.

A prominent deep-RL family uses prediction error as a novelty signal. Early deep predictive-model bonuses reward forward-model misprediction \cite{stadie-2015}. The Intrinsic Curiosity Module (ICM) learns an encoder with inverse and forward dynamics and uses forward error in feature space as intrinsic reward \cite{pathak-2017}, while Random Network Distillation (RND) replaces explicit dynamics prediction with the error of a predictor network trained to match a fixed random target \cite{burda-2018B}. However, raw error can overvalue stochastic or otherwise hard-to-predict transitions (the ``noisy-TV'' effect), and large-scale analyses document brittleness and late-training regressions when curiosity remains active \cite{pathak-2017,burda-2018A}. Episodic novelty and hybrid intrinsic objectives mitigate such traps by emphasizing controllable novelty and revisitation over pure unpredictability \cite{savinov-2018,badia-2020}.

Learning-progress approaches reward improvement rather than surprise. In developmental robotics, IAC and its robust variant R-IAC partition the sensorimotor space and prioritize regions whose prediction error decreases over time \cite{oudeyer-2007B,baranes-2009}. This local notion of progress is closely related to prediction gain and to intrinsic bonuses derived from changing model likelihood under density models \cite{bellemare-2016}. The GLPE family adopts the same principle in a learned latent space using an online adaptive partition and lightweight exponential moving averages of forward-model error, allowing progress to be computed from the on-policy data stream used by PPO (Sections~\ref{sec:method}--\ref{sec:experimental_setup}). GLPE further combines learning progress with a normalized impact term inspired by representation-change bonuses \cite{raileanu-2020} and introduces a region-specific gate that suppresses persistently high-error, low-progress regions, directly targeting the unlearnable-but-attractive regime highlighted in prior critiques \cite{burda-2018A,pathak-2017}. Finally, we evaluate all intrinsic objectives under a common PPO+GAE backbone to isolate their effect in standard on-policy continuous control \cite{schulman-2017,schulman-2015}.
