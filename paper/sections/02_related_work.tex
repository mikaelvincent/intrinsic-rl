\section{Related Work}
\label{sec:related_work}

Intrinsic motivation and curiosity provide reinforcement learning agents with internally generated reward signals that guide exploration when extrinsic feedback is sparse or delayed. Early computational accounts relate curiosity to learning progress by rewarding improvements in prediction or compression and discouraging experience that is either trivial or fundamentally unlearnable \cite{schmidhuber-1991,oudeyer-2007A,oudeyer-2007B,barto-2012}. In modern deep RL, such signals are typically implemented as additive shaping terms combined with task reward; shaping theory provides conditions under which reward transformations preserve optimal policies \cite{ng-1999}.

Exploration has also been formalized through visitation and uncertainty. Classic count-based methods are effective in small or discretized state spaces and have been extended to high-dimensional observations via pseudo-counts from learned density models and hashing/discretization schemes \cite{bellemare-2016,ostrovski-2017,tang-2016}. Because our benchmarks emphasize continuous control with vector observations (Section~\ref{sec:experimental_setup}), we focus on intrinsic signals computed in a learned feature space rather than explicit state counting.

Prediction-error curiosity rewards transitions that are surprising under a learned predictive model. Early deep predictive bonuses \cite{stadie-2015} and the Intrinsic Curiosity Module (ICM) \cite{pathak-2017} learn representations together with forward (and often inverse) dynamics predictors, while Random Network Distillation (RND) measures novelty as prediction error against a fixed random target network \cite{burda-2018B}. Related approaches interpret intrinsic reward as information gain or epistemic uncertainty, using variational Bayesian dynamics models or ensemble/disagreement signals \cite{mohamed-2015,houthooft-2016,pathak-2019}. However, raw prediction error can remain high in stochastic or chaotic regions, leading to the ``noisy-TV'' failure mode where agents chase irreducible surprise and degrade sample efficiency \cite{burda-2018A,burda-2018B,mavor-parker-2021}.

Learning-progress methods address this issue by rewarding improvement rather than absolute error. Region-based approaches from developmental robotics and intrinsically motivated RL estimate progress locally within an adaptive partition and focus exploration on regions where prediction error is decreasing \cite{singh-2004,oudeyer-2007B,baranes-2009}. Our RIAC baseline instantiates this principle in feature space, and the GLPE family follows the same regional progress signal while adding a normalized impact term. GLPE (no gate) uses this combined score directly, while GLPE introduces a region-specific gate that suppresses regions that stay high-error without progress using median-based global thresholds and hysteresis (Section~\ref{sec:method}). This gating is complementary to mechanisms that mitigate stochastic distractions via episodic novelty or directed exploration strategies \cite{savinov-2018,badia-2020}.

Impact-driven exploration provides a complementary notion of novelty grounded in controllable change. RIDE rewards representation change and uses episodic visitation statistics to maintain exploration pressure \cite{raileanu-2020}; our impact term serves a similar role but is combined with learning progress to prioritize learnable regions. Finally, because intrinsic reward design is largely orthogonal to the policy optimizer, we evaluate all methods under a shared PPO+GAE backbone for controlled comparison \cite{schulman-2017,schulman-2015}.
